{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NLezzTSoMcI8"
   },
   "outputs": [],
   "source": [
    "# In order to ignore FutureWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(gpu, \"\\n\")\n",
    "else:\n",
    "  print(\"No GPU device found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun  3 15:06:46 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.98                 Driver Version: 535.98       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 46%   51C    P8              12W / 200W |    556MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1740    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      2856    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6240    C+G   ...0_x64__8wekyb3d8bbwe\\HxAccounts.exe    N/A      |\n",
      "|    0   N/A  N/A      7748    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     10004    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     10832    C+G   ...2.0_x64__cv1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13400    C+G   ...ejd91yc\\AdobeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     15172    C+G   ...soft Office\\root\\Office16\\EXCEL.EXE    N/A      |\n",
      "|    0   N/A  N/A     19796    C+G   ...on\\113.0.1774.57\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     20128    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21336    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     21412    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     22120    C+G   ...on\\113.0.1774.57\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     23028    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     24032    C+G   ...8bbwe\\SnippingTool\\SnippingTool.exe    N/A      |\n",
      "|    0   N/A  N/A     24972    C+G   ...m Files\\Mozilla Firefox\\firefox.exe    N/A      |\n",
      "|    0   N/A  N/A     24984    C+G   ...aam7r\\AcrobatNotificationClient.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzFzH3CAMmxQ",
    "outputId": "ea2fa5c6-b07c-431b-f0a7-cc8884e27930"
   },
   "outputs": [],
   "source": [
    "#Combined Dataset\n",
    "\n",
    "df = pd.read_csv('H:/Datasets/cic-ids-2018/ddos-loic-udp_hoic_21-02-2018.csv', low_memory=True)\n",
    "#df = df.append(pd.read_csv('H:/Datasets/cic-ids-2018/dos-goldeneye-slowloris_15-02-2018.csv', low_memory=True))\n",
    "#df = df.append(pd.read_csv('H:/Datasets/cic-ids-2018/dos-slowhttp-hulk_16-02-2018.csv', low_memory=True))\n",
    "#df = df.append(pd.read_csv('H:/Datasets/cic-ids-2018/bot_02-03-2018.csv', low_memory=True))\n",
    "\n",
    "df = df.rename(columns={'label': 'Label'})\n",
    "#df.drop(df.loc[df['Label'] == 'Label'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "oQdjN1DTBLWl",
    "outputId": "4ad8ccc8-7fc9-42d3-9acf-a4b0663efc6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dst_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>tot_fwd_pkts</th>\n",
       "      <th>tot_bwd_pkts</th>\n",
       "      <th>totlen_fwd_pkts</th>\n",
       "      <th>totlen_bwd_pkts</th>\n",
       "      <th>fwd_pkt_len_max</th>\n",
       "      <th>fwd_pkt_len_min</th>\n",
       "      <th>fwd_pkt_len_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>fwd_act_data_pkts</th>\n",
       "      <th>fwd_seg_size_min</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "      <td>1.048575e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.958764e+04</td>\n",
       "      <td>6.037952e+00</td>\n",
       "      <td>3.990401e+05</td>\n",
       "      <td>1.968099e+02</td>\n",
       "      <td>1.309663e+00</td>\n",
       "      <td>6.563581e+03</td>\n",
       "      <td>2.486527e+02</td>\n",
       "      <td>3.666938e+02</td>\n",
       "      <td>2.549479e-01</td>\n",
       "      <td>7.977144e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.941294e+02</td>\n",
       "      <td>1.995892e+01</td>\n",
       "      <td>7.521672e+03</td>\n",
       "      <td>2.589427e+03</td>\n",
       "      <td>1.003609e+04</td>\n",
       "      <td>5.446253e+03</td>\n",
       "      <td>3.140514e+04</td>\n",
       "      <td>1.602645e+04</td>\n",
       "      <td>5.723070e+04</td>\n",
       "      <td>1.633481e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.719338e+04</td>\n",
       "      <td>6.495848e-01</td>\n",
       "      <td>6.630337e+06</td>\n",
       "      <td>4.899047e+03</td>\n",
       "      <td>1.477423e+00</td>\n",
       "      <td>1.567596e+05</td>\n",
       "      <td>3.257857e+02</td>\n",
       "      <td>4.226960e+02</td>\n",
       "      <td>8.978105e+00</td>\n",
       "      <td>8.511139e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.899113e+03</td>\n",
       "      <td>7.395666e-01</td>\n",
       "      <td>2.033449e+05</td>\n",
       "      <td>1.034448e+05</td>\n",
       "      <td>2.825239e+05</td>\n",
       "      <td>1.568789e+05</td>\n",
       "      <td>8.419967e+05</td>\n",
       "      <td>4.340229e+05</td>\n",
       "      <td>1.421182e+06</td>\n",
       "      <td>5.894628e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.408000e+03</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>4.720000e+03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.470000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.355300e+04</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.464900e+04</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>9.350000e+02</td>\n",
       "      <td>3.160000e+02</td>\n",
       "      <td>9.350000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.870000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.553400e+04</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>3.096290e+05</td>\n",
       "      <td>1.240000e+02</td>\n",
       "      <td>9.908128e+06</td>\n",
       "      <td>9.121000e+03</td>\n",
       "      <td>2.224000e+03</td>\n",
       "      <td>9.760000e+02</td>\n",
       "      <td>9.760000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>3.096280e+05</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>2.000000e+07</td>\n",
       "      <td>1.620000e+07</td>\n",
       "      <td>2.800000e+07</td>\n",
       "      <td>2.000000e+07</td>\n",
       "      <td>1.060000e+08</td>\n",
       "      <td>5.030000e+07</td>\n",
       "      <td>1.060000e+08</td>\n",
       "      <td>1.060000e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           dst_port      protocol  flow_duration  tot_fwd_pkts  tot_bwd_pkts  \\\n",
       "count  1.048575e+06  1.048575e+06   1.048575e+06  1.048575e+06  1.048575e+06   \n",
       "mean   1.958764e+04  6.037952e+00   3.990401e+05  1.968099e+02  1.309663e+00   \n",
       "std    2.719338e+04  6.495848e-01   6.630337e+06  4.899047e+03  1.477423e+00   \n",
       "min    0.000000e+00  0.000000e+00   2.000000e+00  1.000000e+00  0.000000e+00   \n",
       "25%    8.000000e+01  6.000000e+00   1.408000e+03  2.000000e+00  0.000000e+00   \n",
       "50%    8.000000e+01  6.000000e+00   4.720000e+03  3.000000e+00  0.000000e+00   \n",
       "75%    5.355300e+04  6.000000e+00   1.464900e+04  5.000000e+00  2.000000e+00   \n",
       "max    6.553400e+04  1.700000e+01   1.200000e+08  3.096290e+05  1.240000e+02   \n",
       "\n",
       "       totlen_fwd_pkts  totlen_bwd_pkts  fwd_pkt_len_max  fwd_pkt_len_min  \\\n",
       "count     1.048575e+06     1.048575e+06     1.048575e+06     1.048575e+06   \n",
       "mean      6.563581e+03     2.486527e+02     3.666938e+02     2.549479e-01   \n",
       "std       1.567596e+05     3.257857e+02     4.226960e+02     8.978105e+00   \n",
       "min       0.000000e+00     0.000000e+00     0.000000e+00     0.000000e+00   \n",
       "25%       0.000000e+00     0.000000e+00     0.000000e+00     0.000000e+00   \n",
       "50%       2.470000e+02     0.000000e+00     3.200000e+01     0.000000e+00   \n",
       "75%       9.350000e+02     3.160000e+02     9.350000e+02     0.000000e+00   \n",
       "max       9.908128e+06     9.121000e+03     2.224000e+03     9.760000e+02   \n",
       "\n",
       "       fwd_pkt_len_mean  ...  fwd_act_data_pkts  fwd_seg_size_min  \\\n",
       "count      1.048575e+06  ...       1.048575e+06      1.048575e+06   \n",
       "mean       7.977144e+01  ...       1.941294e+02      1.995892e+01   \n",
       "std        8.511139e+01  ...       4.899113e+03      7.395666e-01   \n",
       "min        0.000000e+00  ...       0.000000e+00      0.000000e+00   \n",
       "25%        0.000000e+00  ...       0.000000e+00      2.000000e+01   \n",
       "50%        3.200000e+01  ...       1.000000e+00      2.000000e+01   \n",
       "75%        1.870000e+02  ...       1.000000e+00      2.000000e+01   \n",
       "max        9.760000e+02  ...       3.096280e+05      4.400000e+01   \n",
       "\n",
       "        active_mean    active_std    active_max    active_min     idle_mean  \\\n",
       "count  1.048575e+06  1.048575e+06  1.048575e+06  1.048575e+06  1.048575e+06   \n",
       "mean   7.521672e+03  2.589427e+03  1.003609e+04  5.446253e+03  3.140514e+04   \n",
       "std    2.033449e+05  1.034448e+05  2.825239e+05  1.568789e+05  8.419967e+05   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    2.000000e+07  1.620000e+07  2.800000e+07  2.000000e+07  1.060000e+08   \n",
       "\n",
       "           idle_std      idle_max      idle_min  \n",
       "count  1.048575e+06  1.048575e+06  1.048575e+06  \n",
       "mean   1.602645e+04  5.723070e+04  1.633481e+04  \n",
       "std    4.340229e+05  1.421182e+06  5.894628e+05  \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "max    5.030000e+07  1.060000e+08  1.060000e+08  \n",
       "\n",
       "[8 rows x 78 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "YZFMTRPvM9ts",
    "outputId": "03894b34-3289-416f-8ff7-f38b6852d6f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dst_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>tot_fwd_pkts</th>\n",
       "      <th>tot_bwd_pkts</th>\n",
       "      <th>totlen_fwd_pkts</th>\n",
       "      <th>totlen_bwd_pkts</th>\n",
       "      <th>fwd_pkt_len_max</th>\n",
       "      <th>fwd_pkt_len_min</th>\n",
       "      <th>...</th>\n",
       "      <th>fwd_seg_size_min</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>21/02/2018 08:33:25</td>\n",
       "      <td>37953</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>127</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>17</td>\n",
       "      <td>21/02/2018 08:33:06</td>\n",
       "      <td>117573474</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58800000.0</td>\n",
       "      <td>23800000.0</td>\n",
       "      <td>75600000</td>\n",
       "      <td>42000000</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>17</td>\n",
       "      <td>21/02/2018 08:33:06</td>\n",
       "      <td>117573474</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58800000.0</td>\n",
       "      <td>23800000.0</td>\n",
       "      <td>75600000</td>\n",
       "      <td>42000000</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>17</td>\n",
       "      <td>21/02/2018 08:33:11</td>\n",
       "      <td>99743998</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>4000290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000290</td>\n",
       "      <td>4000290</td>\n",
       "      <td>31900000.0</td>\n",
       "      <td>37900000.0</td>\n",
       "      <td>75600000</td>\n",
       "      <td>7200397</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>17</td>\n",
       "      <td>21/02/2018 08:33:11</td>\n",
       "      <td>99743999</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>4000286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000286</td>\n",
       "      <td>4000286</td>\n",
       "      <td>31900000.0</td>\n",
       "      <td>37900000.0</td>\n",
       "      <td>75600000</td>\n",
       "      <td>7200399</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dst_port  protocol            timestamp  flow_duration  tot_fwd_pkts  \\\n",
       "0        80         6  21/02/2018 08:33:25          37953             5   \n",
       "1       500        17  21/02/2018 08:33:06      117573474             3   \n",
       "2       500        17  21/02/2018 08:33:06      117573474             3   \n",
       "3       500        17  21/02/2018 08:33:11       99743998             5   \n",
       "4       500        17  21/02/2018 08:33:11       99743999             5   \n",
       "\n",
       "   tot_bwd_pkts  totlen_fwd_pkts  totlen_bwd_pkts  fwd_pkt_len_max  \\\n",
       "0             3              135              127              135   \n",
       "1             0             1500                0              500   \n",
       "2             0             1500                0              500   \n",
       "3             0             2500                0              500   \n",
       "4             0             2500                0              500   \n",
       "\n",
       "   fwd_pkt_len_min  ...  fwd_seg_size_min  active_mean  active_std  \\\n",
       "0                0  ...                32          0.0         0.0   \n",
       "1              500  ...                 8          0.0         0.0   \n",
       "2              500  ...                 8          0.0         0.0   \n",
       "3              500  ...                 8    4000290.0         0.0   \n",
       "4              500  ...                 8    4000286.0         0.0   \n",
       "\n",
       "   active_max  active_min   idle_mean    idle_std  idle_max  idle_min   Label  \n",
       "0           0           0         0.0         0.0         0         0  Benign  \n",
       "1           0           0  58800000.0  23800000.0  75600000  42000000  Benign  \n",
       "2           0           0  58800000.0  23800000.0  75600000  42000000  Benign  \n",
       "3     4000290     4000290  31900000.0  37900000.0  75600000   7200397  Benign  \n",
       "4     4000286     4000286  31900000.0  37900000.0  75600000   7200399  Benign  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDOS attack-HOIC        686012\n",
       "Benign                  360833\n",
       "DDOS attack-LOIC-UDP      1730\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploratory data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "class_distribution = df[df[\"Label\"] != \"Benign\"][\"Label\"].value_counts()\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Data points per Class')\n",
    "plt.title('Distribution of data without Benign')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HixtSvMpM_Fq"
   },
   "outputs": [],
   "source": [
    "metadata = ['fl_dur' #Flow duration\n",
    ",'tot_fw_pk' #Total packets in the forward direction\n",
    ",'tot_bw_pk' #Total packets in the backward direction\n",
    ",'tot_l_fw_pkt' #Total size of packet in forward direction\n",
    ",'fw_pkt_l_max' #Maximum size of packet in forward direction\n",
    ",'fw_pkt_l_min' #Minimum size of packet in forward direction\n",
    ",'fw_pkt_l_avg' #Average size of packet in forward direction\n",
    ",'fw_pkt_l_std' #Standard deviation size of packet in forward direction\n",
    ",'Bw_pkt_l_max' #Maximum size of packet in backward direction\n",
    ",'Bw_pkt_l_min' #Minimum size of packet in backward direction\n",
    ",'Bw_pkt_l_avg' #Mean size of packet in backward direction\n",
    ",'Bw_pkt_l_std' #Standard deviation size of packet in backward direction\n",
    ",'fl_byt_s' #flow byte rate that is number of packets transferred per second\n",
    ",'fl_pkt_s' #flow packets rate that is number of packets transferred per second\n",
    ",'fl_iat_avg' #Average time between two flows\n",
    ",'fl_iat_std' #Standard deviation time two flows\n",
    ",'fl_iat_max' #Maximum time between two flows\n",
    ",'fl_iat_min' #Minimum time between two flows\n",
    ",'fw_iat_tot' #Total time between two packets sent in the forward direction\n",
    ",'fw_iat_avg' #Mean time between two packets sent in the forward direction\n",
    ",'fw_iat_std' #Standard deviation time between two packets sent in the forward direction\n",
    ",'fw_iat_max' #Maximum time between two packets sent in the forward direction\n",
    ",'fw_iat_min' #Minimum time between two packets sent in the forward direction\n",
    ",'bw_iat_tot' #Total time between two packets sent in the backward direction\n",
    ",'bw_iat_avg' #Mean time between two packets sent in the backward direction\n",
    ",'bw_iat_std' #Standard deviation time between two packets sent in the backward direction\n",
    ",'bw_iat_max' #Maximum time between two packets sent in the backward direction\n",
    ",'bw_iat_min' #Minimum time between two packets sent in the backward direction\n",
    ",'fw_psh_flag' #Number of times the PSH flag was set in packets travelling in the forward direction (0 for UDP)\n",
    ",'bw_psh_flag' #Number of times the PSH flag was set in packets travelling in the backward direction (0 for UDP)\n",
    ",'fw_urg_flag' #Number of times the URG flag was set in packets travelling in the forward direction (0 for UDP)\n",
    ",'bw_urg_flag' #Number of times the URG flag was set in packets travelling in the backward direction (0 for UDP)\n",
    ",'fw_hdr_len' #Total bytes used for headers in the forward direction\n",
    ",'bw_hdr_len' #Total bytes used for headers in the forward direction\n",
    ",'fw_pkt_s' #Number of forward packets per second\n",
    ",'bw_pkt_s' #Number of backward packets per second\n",
    ",'pkt_len_min' #Minimum length of a flow\n",
    ",'pkt_len_max' #Maximum length of a flow\n",
    ",'pkt_len_avg' #Mean length of a flow\n",
    ",'pkt_len_std' #Standard deviation length of a flow\n",
    ",'pkt_len_va' #Minimum inter-arrival time of packet\n",
    ",'fin_cnt' #Number of packets with FIN\n",
    ",'syn_cnt' #Number of packets with SYN\n",
    ",'rst_cnt' #Number of packets with RST\n",
    ",'pst_cnt' #Number of packets with PUSH\n",
    ",'ack_cnt' #Number of packets with ACK\n",
    ",'urg_cnt' #Number of packets with URG\n",
    ",'cwe_cnt' #Number of packets with CWE\n",
    ",'ece_cnt' #Number of packets with ECE\n",
    ",'down_up_ratio' #Download and upload ratio\n",
    ",'pkt_size_avg' #Average size of packet\n",
    ",'fw_seg_avg' #Average size observed in the forward direction\n",
    ",'bw_seg_avg' #Average size observed in the backward direction\n",
    ",'fw_byt_blk_avg' #Average number of bytes bulk rate in the forward direction\n",
    ",'fw_pkt_blk_avg' #Average number of packets bulk rate in the forward direction\n",
    ",'fw_blk_rate_avg' #Average number of bulk rate in the forward direction\n",
    ",'bw_byt_blk_avg' #Average number of bytes bulk rate in the backward direction\n",
    ",'bw_pkt_blk_avg' #Average number of packets bulk rate in the backward direction\n",
    ",'bw_blk_rate_avg' #Average number of bulk rate in the backward direction\n",
    ",'subfl_fw_pk' #The average number of packets in a sub flow in the forward direction\n",
    ",'subfl_fw_byt' #The average number of bytes in a sub flow in the forward direction\n",
    ",'subfl_bw_pkt' #The average number of packets in a sub flow in the backward direction\n",
    ",'subfl_bw_byt' #The average number of bytes in a sub flow in the backward direction\n",
    ",'fw_win_byt' #Number of bytes sent in initial window in the forward direction\n",
    ",'bw_win_byt' ## of bytes sent in initial window in the backward direction\n",
    ",'Fw_act_pkt' ## of packets with at least 1 byte of TCP data payload in the forward direction\n",
    ",'fw_seg_min' #Minimum segment size observed in the forward direction\n",
    ",'atv_avg' #Mean time a flow was active before becoming idle\n",
    ",'atv_std' #Standard deviation time a flow was active before becoming idle\n",
    ",'atv_max' #Maximum time a flow was active before becoming idle\n",
    ",'atv_min' #Minimum time a flow was active before becoming idle\n",
    ",'idl_avg' #Mean time a flow was idle before becoming active\n",
    ",'idl_std' #Standard deviation time a flow was idle before becoming active\n",
    ",'idl_max' #Maximum time a flow was idle before becoming active\n",
    ",'idl_min' #Minimum time a flow was idle before becoming active\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9ZjigalNCIZ",
    "outputId": "69220177-4636-41ab-d919-78b661002440"
   },
   "outputs": [],
   "source": [
    "#features = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TZXxDViHNFt7"
   },
   "outputs": [],
   "source": [
    "features = ['Flow Duration', 'Tot Fwd Pkts',\n",
    "       'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
    "       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
    "       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
    "       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
    "       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
    "       'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
    "       'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
    "       'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
    "       'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
    "       'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
    "       'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
    "       'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
    "       'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
    "       'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
    "       'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
    "       'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
    "       'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
    "       'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
    "       'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
    "       'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping some columns: \n",
      "\t there are 77 columns and 1048575 rows\n"
     ]
    }
   ],
   "source": [
    "# Remove columns with only values of 0\n",
    "useless_columns = ['dst_port', 'protocol', 'timestamp']\n",
    "df.drop(labels=useless_columns, axis='columns', inplace=True)\n",
    "print('After dropping some columns: \\n\\t there are {} columns and {} rows'.format(len(df.columns), len(df)))\n",
    "\n",
    "features = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "\n",
    "\n",
    "#drop na values and reset index\n",
    "data_clean = df.dropna().reset_index()\n",
    "\n",
    "# label encoding\n",
    "labelencoder = LabelEncoder()\n",
    "data_clean['Label'] = labelencoder.fit_transform(data_clean['Label'])\n",
    "\n",
    "data_clean['Label'].value_counts()\n",
    "\n",
    "data_np = data_clean.to_numpy(dtype=\"float32\")\n",
    "data_np = data_np[~np.isinf(data_np).any(axis=1)]\n",
    "\n",
    "#del df\n",
    "\n",
    "X = data_np[:, 0:76]\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "Y = enc.fit_transform(data_np[:,77:]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 78)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'flow_duration', 'tot_fwd_pkts', 'tot_bwd_pkts',\n",
       "       'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max',\n",
       "       'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std',\n",
       "       'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean',\n",
       "       'bwd_pkt_len_std', 'flow_byts_s', 'flow_pkts_s', 'flow_iat_mean',\n",
       "       'flow_iat_std', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_tot',\n",
       "       'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max', 'fwd_iat_min',\n",
       "       'bwd_iat_tot', 'bwd_iat_mean', 'bwd_iat_std', 'bwd_iat_max',\n",
       "       'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags',\n",
       "       'bwd_urg_flags', 'fwd_header_len', 'bwd_header_len', 'fwd_pkts_s',\n",
       "       'bwd_pkts_s', 'pkt_len_min', 'pkt_len_max', 'pkt_len_mean',\n",
       "       'pkt_len_std', 'pkt_len_var', 'fin_flag_cnt', 'syn_flag_cnt',\n",
       "       'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt',\n",
       "       'cwe_flag_count', 'ece_flag_cnt', 'down_up_ratio', 'pkt_size_avg',\n",
       "       'fwd_seg_size_avg', 'bwd_seg_size_avg', 'fwd_byts_b_avg',\n",
       "       'fwd_pkts_b_avg', 'fwd_blk_rate_avg', 'bwd_byts_b_avg',\n",
       "       'bwd_pkts_b_avg', 'bwd_blk_rate_avg', 'subflow_fwd_pkts',\n",
       "       'subflow_fwd_byts', 'subflow_bwd_pkts', 'subflow_bwd_byts',\n",
       "       'init_fwd_win_byts', 'init_bwd_win_byts', 'fwd_act_data_pkts',\n",
       "       'fwd_seg_size_min', 'active_mean', 'active_std', 'active_max',\n",
       "       'active_min', 'idle_mean', 'idle_std', 'idle_max', 'idle_min', 'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>tot_fwd_pkts</th>\n",
       "      <th>tot_bwd_pkts</th>\n",
       "      <th>totlen_fwd_pkts</th>\n",
       "      <th>totlen_bwd_pkts</th>\n",
       "      <th>fwd_pkt_len_max</th>\n",
       "      <th>fwd_pkt_len_min</th>\n",
       "      <th>fwd_pkt_len_mean</th>\n",
       "      <th>fwd_pkt_len_std</th>\n",
       "      <th>...</th>\n",
       "      <th>fwd_seg_size_min</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37953</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>127</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>60.373835</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>117573474</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58800000.0</td>\n",
       "      <td>23800000.0</td>\n",
       "      <td>75600000</td>\n",
       "      <td>42000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>117573474</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58800000.0</td>\n",
       "      <td>23800000.0</td>\n",
       "      <td>75600000</td>\n",
       "      <td>42000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>99743998</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>4000290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000290</td>\n",
       "      <td>4000290</td>\n",
       "      <td>31900000.0</td>\n",
       "      <td>37900000.0</td>\n",
       "      <td>75600000</td>\n",
       "      <td>7200397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>99743999</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>4000286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000286</td>\n",
       "      <td>4000286</td>\n",
       "      <td>31900000.0</td>\n",
       "      <td>37900000.0</td>\n",
       "      <td>75600000</td>\n",
       "      <td>7200399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  flow_duration  tot_fwd_pkts  tot_bwd_pkts  totlen_fwd_pkts  \\\n",
       "0      0          37953             5             3              135   \n",
       "1      1      117573474             3             0             1500   \n",
       "2      2      117573474             3             0             1500   \n",
       "3      3       99743998             5             0             2500   \n",
       "4      4       99743999             5             0             2500   \n",
       "\n",
       "   totlen_bwd_pkts  fwd_pkt_len_max  fwd_pkt_len_min  fwd_pkt_len_mean  \\\n",
       "0              127              135                0              27.0   \n",
       "1                0              500              500             500.0   \n",
       "2                0              500              500             500.0   \n",
       "3                0              500              500             500.0   \n",
       "4                0              500              500             500.0   \n",
       "\n",
       "   fwd_pkt_len_std  ...  fwd_seg_size_min  active_mean  active_std  \\\n",
       "0        60.373835  ...                32          0.0         0.0   \n",
       "1         0.000000  ...                 8          0.0         0.0   \n",
       "2         0.000000  ...                 8          0.0         0.0   \n",
       "3         0.000000  ...                 8    4000290.0         0.0   \n",
       "4         0.000000  ...                 8    4000286.0         0.0   \n",
       "\n",
       "   active_max  active_min   idle_mean    idle_std  idle_max  idle_min  Label  \n",
       "0           0           0         0.0         0.0         0         0      0  \n",
       "1           0           0  58800000.0  23800000.0  75600000  42000000      0  \n",
       "2           0           0  58800000.0  23800000.0  75600000  42000000      0  \n",
       "3     4000290     4000290  31900000.0  37900000.0  75600000   7200397      0  \n",
       "4     4000286     4000286  31900000.0  37900000.0  75600000   7200399      0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.25, random_state=2, shuffle=True)\n",
    "\n",
    "_features = X.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786431, 76)\n",
      "(786431, 3)\n",
      "(262144, 76)\n",
      "(262144, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 76)\n",
      "(1048575, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to ignore FutureWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                4928      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,567\n",
      "Trainable params: 79,567\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "6144/6144 [==============================] - 20s 3ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 1.7273e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 9.8169e-05 - accuracy: 1.0000 - val_loss: 1.9684e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 6.0374e-05 - accuracy: 1.0000 - val_loss: 2.7372e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 5.9699e-05 - accuracy: 1.0000 - val_loss: 7.0081e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 1.5435e-05 - accuracy: 1.0000 - val_loss: 9.3064e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 6.8171e-06 - accuracy: 1.0000 - val_loss: 8.6589e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 6.7193e-05 - accuracy: 1.0000 - val_loss: 9.6728e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 2.9434e-06 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 1.4664e-06 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 3.8073e-05 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 6.8360e-07 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 2.9397e-06 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 8.6199e-06 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 1.8250e-07 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 2.0798e-05 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 4.7544e-06 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 5.7452e-06 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 2.2517e-05 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 1.9065e-06 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 9.6436e-06 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "6144/6144 [==============================] - 20s 3ms/step - loss: 3.4544e-08 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 3.0339e-06 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 5.7502e-09 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 6.0274e-06 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 4.9115e-06 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "6144/6144 [==============================] - 18s 3ms/step - loss: 1.1598e-06 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 1.0818e-05 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 3.2798e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 1.2478e-08 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "6144/6144 [==============================] - 19s 3ms/step - loss: 1.0129e-08 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, input_dim=_features, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(n_classes, kernel_initializer='normal'))\n",
    "model.add(Dense(n_classes, activation = 'softmax'))\n",
    "\n",
    "model.summary() \n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss='BinaryCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  patience=3, min_lr=0.0001)\n",
    "    \n",
    "history = model.fit(X_train, Y_train,\n",
    "                              batch_size=128,\n",
    "                              epochs=30,\n",
    "                              verbose=True, #callbacks=[reduce_lr, early_stop_callback],\n",
    "                              validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('H:/Datasets/CIC-IDS2018/dnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192/8192 [==============================] - 7s 827us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_test = Y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262144,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 89827,      1,      0],\n",
       "       [     0, 171853,      0],\n",
       "       [     0,      0,    463]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sn\n",
    "\n",
    "confMat = confusion_matrix(y_test, pred)\n",
    "confMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confMat\n",
    "\n",
    "labels = ['DDOS attack-HOIC','Beneig', 'DDOS attack-LOIC-UDP']\n",
    "\n",
    "sn.heatmap(cf_matrix / np.sum(cf_matrix), annot=True, fmt='.2%', xticklabels=labels, yticklabels=labels, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192/8192 [==============================] - 13s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "8192/8192 [==============================] - 7s 835us/step\n",
      "Inference time: 9.04 seconds\n",
      "Completed\n",
      "Time taken: 0:00:22.880786\n",
      "Validation score: 0.9999961853027344\n",
      "Evaluation score: [0.0031947544775903225, 0.9999961853027344]\n",
      "Recall score: 0.9999961853027344\n",
      "Precision score: 0.9999961853249317\n",
      "F1 Measure score: 0.999996185297667\n",
      "ROC-AUC score: 0.9999962987187817\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "escore = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "print(\"Inference time: {:.2f} seconds\".format(inference_time))\n",
    "\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(Y_test,axis=1)\n",
    "\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "rscore = recall_score(y_eval, pred, average='weighted')\n",
    "ascore = precision_score(y_eval, pred, average='weighted')\n",
    "f1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) / (precision + recall) for manual\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_eval)\n",
    "y_eval = lb.transform(y_eval)\n",
    "pred = lb.transform(pred)\n",
    "roc_score = roc_auc_score(y_eval, pred)\n",
    "\n",
    "print('Completed')\n",
    "print('Time taken:',dt.datetime.now()-start)\n",
    "\n",
    "print(\"Validation score: {}\".format(score))\n",
    "print(\"Evaluation score: {}\".format(escore))\n",
    "print(\"Recall score: {}\".format(rscore))\n",
    "print(\"Precision score: {}\".format(ascore))\n",
    "print(\"F1 Measure score: {}\".format(f1score))\n",
    "print(\"ROC-AUC score: {}\".format(roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for training and validation loss\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_by_epoch = 1\n",
    "epochs = range(start_by_epoch, len(loss_values) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "plt.plot(epochs, acc[start_by_epoch-1:], label='Training accuracy')\n",
    "plt.plot(epochs, val_acc[start_by_epoch-1:], label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "#plt.subplot(212)\n",
    "\n",
    "plt.plot(epochs, loss_values[start_by_epoch-1:], label='Training Loss')\n",
    "plt.plot(epochs, val_loss_values[start_by_epoch-1:], label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(-0.02,1.02)\n",
    "plt.title('Training and Validation Rate')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(confMat)\n",
    "\n",
    "labels = ['DDOS attack-HOIC','Beneig', 'DDOS attack-LOIC-UDP']\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(cm_df, annot=True, annot_kws={\"size\":12}, xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
    "\n",
    "#sn.heatmap(cm_df, annot=True, annot_kws={\"size\":12}, fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 76, 128)           896       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 38, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 38, 256)           196864    \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 19, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4864)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               1245440   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,443,983\n",
      "Trainable params: 1,443,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "6144/6144 [==============================] - 25s 4ms/step - loss: 0.0094 - accuracy: 0.9993 - val_loss: 1.9844e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 1.7387e-05 - accuracy: 1.0000 - val_loss: 1.2756e-06 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 2.0622e-04 - accuracy: 1.0000 - val_loss: 3.6869e-07 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.5647e-07 - accuracy: 1.0000 - val_loss: 8.7192e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.3416e-07 - accuracy: 1.0000 - val_loss: 6.6632e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "6144/6144 [==============================] - 22s 4ms/step - loss: 5.0889e-09 - accuracy: 1.0000 - val_loss: 3.1649e-10 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.7272e-10 - accuracy: 1.0000 - val_loss: 3.1740e-10 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 5.6433e-10 - accuracy: 1.0000 - val_loss: 6.8212e-11 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.0723e-10 - accuracy: 1.0000 - val_loss: 7.7307e-12 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.2149e-10 - accuracy: 1.0000 - val_loss: 1.0004e-11 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.0468e-11 - accuracy: 1.0000 - val_loss: 7.2760e-12 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 1.3491e-11 - accuracy: 1.0000 - val_loss: 5.9117e-12 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 8.8673e-11 - accuracy: 1.0000 - val_loss: 1.3642e-12 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "6144/6144 [==============================] - 24s 4ms/step - loss: 5.4570e-12 - accuracy: 1.0000 - val_loss: 1.8190e-12 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 1.1672e-11 - accuracy: 1.0000 - val_loss: 1.3642e-12 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 2.8801e-12 - accuracy: 1.0000 - val_loss: 1.3642e-12 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 5.8510e-11 - accuracy: 1.0000 - val_loss: 4.5475e-13 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.7896e-12 - accuracy: 1.0000 - val_loss: 4.5475e-13 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 2.4253e-12 - accuracy: 1.0000 - val_loss: 4.5475e-13 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.7896e-12 - accuracy: 1.0000 - val_loss: 4.5475e-13 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.4864e-12 - accuracy: 1.0000 - val_loss: 4.5475e-13 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 6.6696e-12 - accuracy: 1.0000 - val_loss: 4.5475e-13 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "6144/6144 [==============================] - 24s 4ms/step - loss: 1.8948e-11 - accuracy: 1.0000 - val_loss: 9.0949e-13 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 4.2443e-12 - accuracy: 1.0000 - val_loss: 9.0949e-13 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 1.8493e-11 - accuracy: 1.0000 - val_loss: 9.0949e-13 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 5.4570e-12 - accuracy: 1.0000 - val_loss: 9.0949e-13 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 3.0317e-12 - accuracy: 1.0000 - val_loss: 9.0949e-13 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 1.2127e-12 - accuracy: 1.0000 - val_loss: 9.0949e-13 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 6.8212e-12 - accuracy: 1.0000 - val_loss: 4.5475e-13 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "6144/6144 [==============================] - 23s 4ms/step - loss: 4.5475e-13 - accuracy: 1.0000 - val_loss: 4.5475e-13 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# In order to ignore FutureWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, MaxPooling1D, Dropout, Reshape\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 76, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 76, 1).astype('float32')\n",
    "\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(filters=128, kernel_size=6, padding=\"same\", activation=\"relu\", input_shape=(_features, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Convolution1D(filters=256, kernel_size=6, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(n_classes, kernel_initializer='normal'))\n",
    "model.add(Dense(n_classes, activation = 'softmax'))\n",
    "\n",
    "model.summary() \n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "early_stop_callback = keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  patience=3, min_lr=0.0001)\n",
    "    \n",
    "history = model.fit(X_train, Y_train,\n",
    "                              batch_size=128,\n",
    "                              epochs=30,\n",
    "                              verbose=True, #callbacks=[reduce_lr, early_stop_callback],\n",
    "                              validation_data=(X_test, Y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192/8192 [==============================] - 8s 914us/step\n",
      "Best loss: 4.547473237814098e-13\n",
      "Balanced Acc loss: 100.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "balanced_score = metrics.balanced_accuracy_score(np.argmax(Y_test, axis=1), y_pred) * 100\n",
    "    \n",
    "best_loss = np.amin(history.history['val_loss']) \n",
    "print('Best loss: {}'.format(best_loss))\n",
    "print('Balanced Acc loss: {}'.format(balanced_score))\n",
    "\n",
    "#model.save('H:/Datasets/CIC-IDS2018/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for training and validation loss\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192/8192 [==============================] - 15s 2ms/step - loss: 4.5475e-13 - accuracy: 1.0000\n",
      "8192/8192 [==============================] - 8s 925us/step\n",
      "Inference time: 9.91 seconds\n",
      "Completed\n",
      "Time taken: 0:00:24.762975\n",
      "Validation score: 1.0\n",
      "Evaluation score: [4.547473237814098e-13, 1.0]\n",
      "Recall score: 1.0\n",
      "Precision score: 1.0\n",
      "F1 Measure score: 1.0\n",
      "ROC-AUC score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "escore = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "print(\"Inference time: {:.2f} seconds\".format(inference_time))\n",
    "\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(Y_test,axis=1)\n",
    "\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "rscore = recall_score(y_eval, pred, average='weighted')\n",
    "ascore = precision_score(y_eval, pred, average='weighted')\n",
    "f1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) / (precision + recall) for manual\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_eval)\n",
    "y_eval = lb.transform(y_eval)\n",
    "pred = lb.transform(pred)\n",
    "roc_score = roc_auc_score(y_eval, pred)\n",
    "\n",
    "print('Completed')\n",
    "print('Time taken:',dt.datetime.now()-start)\n",
    "\n",
    "print(\"Validation score: {}\".format(score))\n",
    "print(\"Evaluation score: {}\".format(escore))\n",
    "print(\"Recall score: {}\".format(rscore))\n",
    "print(\"Precision score: {}\".format(ascore))\n",
    "print(\"F1 Measure score: {}\".format(f1score))\n",
    "print(\"ROC-AUC score: {}\".format(roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192/8192 [==============================] - 8s 952us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_test = Y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262144,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 89828,      0,      0],\n",
       "       [     0, 171853,      0],\n",
       "       [     0,      0,    463]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sn\n",
    "\n",
    "confMat = confusion_matrix(y_test, pred)\n",
    "confMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confMat\n",
    "\n",
    "labels = ['DDOS attack-HOIC','Beneig', 'DDOS attack-LOIC-UDP']\n",
    "\n",
    "sn.heatmap(cf_matrix / np.sum(cf_matrix), annot=True, fmt='.2%', xticklabels=labels, yticklabels=labels, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_by_epoch = 1\n",
    "epochs = range(start_by_epoch, len(loss_values) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc[start_by_epoch-1:], label='Training accuracy')\n",
    "plt.plot(epochs, val_acc[start_by_epoch-1:], label='Validation accuracy')\n",
    "plt.title('CNN: Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.subplot(212)\n",
    "\n",
    "plt.plot(epochs, loss_values[start_by_epoch-1:], label='Training Loss')\n",
    "plt.plot(epochs, val_loss_values[start_by_epoch-1:], label='Validation Loss')\n",
    "plt.title('CNN: Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(-0.02,1.02)\n",
    "plt.title('CNN: Training and Validation Rate')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(confMat)\n",
    "\n",
    "labels = ['DDOS attack-HOIC','Beneig', 'DDOS attack-LOIC-UDP']\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(cm_df, annot=True, annot_kws={\"size\":12}, xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
    "\n",
    "#sn.heatmap(cm_df, annot=True, annot_kws={\"size\":12}, fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "IDS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
